{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kohonen map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Цель: выделить основные темы (представленные в виде набора слов), которые встречаются в коллекции документов, с помощью MiniSom. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting MiniSom\n",
      "  Downloading MiniSom-2.2.7.tar.gz (8.1 kB)\n",
      "Building wheels for collected packages: MiniSom\n",
      "  Building wheel for MiniSom (setup.py): started\n",
      "  Building wheel for MiniSom (setup.py): finished with status 'done'\n",
      "  Created wheel for MiniSom: filename=MiniSom-2.2.7-py3-none-any.whl size=8613 sha256=ce77a29263cb14f4400603f562b90bf2a505fbea0c62a9d230b5c225febb58f6\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\0e\\26\\1a\\6859e03682075865e482f052bf299f5de86d27fd9891dc2717\n",
      "Successfully built MiniSom\n",
      "Installing collected packages: MiniSom\n",
      "Successfully installed MiniSom-2.2.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install MiniSom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from minisom import MiniSom\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет - набор данных 20newsgroups - это коллекция примерно из 20000 новостных документов, разделенная (приблизительно) равномерно между 20 различными категориями. \n",
    "\n",
    "https://www.kaggle.com/crawford/20-newsgroups\n",
    "\n",
    "Загрузим набор данных с помощью sklearn и преобразуем текстовые документы в матрицу D, где каждая строка представляет сообщение с использованием представления TF-IDF.\n",
    "\n",
    "### TF-IDF\n",
    "TF-IDF - это  term frequency-inverse document frequency или частотность терминов-обратная частотность документов.\n",
    "\n",
    "Это простой и удобный способ оценить важность термина для какого-либо документа относительно всех остальных документов. Принцип такой — если слово встречается в каком-либо документе часто, при этом встречаясь редко во всех остальных документах — это слово имеет большую значимость для того самого документа.\n",
    "\n",
    "\n",
    "Тerm Frequency - TF  — это частотность термина, которая измеряет, насколько часто термин встречается в документе. Логично предположить, что в длинных документах термин может встретиться в больших количествах, чем в коротких, поэтому абсолютные числа тут не катят. Поэтому применяют относительные — делят количество раз, когда нужный термин встретился в тексте, на общее количество слов в тексте.\n",
    "\n",
    "TF термина а = (Количество раз, когда термин а встретился в тексте / количество всех слов в тексте)\n",
    "\n",
    "IDF — это обратная частотность документов. Она измеряет непосредственно важность термина. То есть, когда мы считали TF, все термины считаются как бы равными по важности друг другу. Но всем известно, что, например, предлоги встречаются очень часто, хотя практически не влияют на смысл текста. И что с этим поделать? Ответ прост — посчитать IDF. Он считается как логарифм от общего количества документов, делённого на количество документов, в которых встречается термин а.\n",
    "\n",
    "IDF термина а = логарифм(Общее количество документов / Количество документов, в которых встречается термин а)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=no_features,\n",
    "                                   stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "D = tfidf.todense().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нам нужно обучить SOM, который кластеризует документы, общее количество нейронов в SOM будет также количеством тем для извлечения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons = 2 # на вход подается двухмерный вектор\n",
    "m_neurons = 4 # сеть будет размером 4x4 \n",
    "# Т.е. каждый нейрон представляет собой n-мерный вектор-столбец, \n",
    "# где n определяется размерностью исходного пространства (размерностью входных векторов).\n",
    "som = MiniSom(n_neurons, m_neurons, no_features)\n",
    "som.pca_weights_init(D)\n",
    "som.train(D, 40000, random_order=False, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем рассматривать в качестве темы список первых десяти top_keywords, связанных с наибольшими весами каждого нейрона. \n",
    "\n",
    "С помощью следующего цикла for мы проверим все веса и восстановим слова, связанные с весами, используя имена функций, сохраненные TfidfVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 : low reported think want truth knowledge shall right people don\n",
      "Topic 2 : don clipper games like armenians people turkey turkish space armenia\n",
      "Topic 3 : generally statement lots dos true money machines dod os like\n",
      "Topic 4 : service cards scsi matter 17 know deleted 19 stuff cable\n",
      "Topic 5 : finally ed email ideas home people didn pick alt learn\n",
      "Topic 6 : used better mail buy x11 pc info thanks appreciated advance\n",
      "Topic 7 : light religion administration expect money like man encryption clinton jesus\n",
      "Topic 8 : mail stuff know jesus card state book thank report mode\n"
     ]
    }
   ],
   "source": [
    "top_keywords = 10\n",
    "\n",
    "weights = som.get_weights()\n",
    "cnt = 1\n",
    "for i in range(n_neurons):\n",
    "    for j in range(m_neurons):\n",
    "        keywords_idx = np.argsort(weights[i,j,:])[-top_keywords:]\n",
    "        keywords = ' '.join([tfidf_feature_names[k] for k in keywords_idx])\n",
    "        print('Topic', cnt, ':', keywords)\n",
    "        cnt += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
